### Transforemr `2017` 

[atttention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)  
[Transformer](http://jalammar.github.io/illustrated-transformer/)  
[pytorch_harvard](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  
[pytorch_kor](https://colab.research.google.com/github/metamath1/ml-simple-works/blob/master/transformer/annotated_transformer.ipynb#scrollTo=zxe2mijnWm9e)



